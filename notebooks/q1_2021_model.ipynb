{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# coding=utf-8\r\n",
    "# Copyright 2021 DAF Trucks NV.\r\n",
    "#\r\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
    "# you may not use this file except in compliance with the License.\r\n",
    "# You may obtain a copy of the License at\r\n",
    "#\r\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\r\n",
    "#\r\n",
    "# Unless required by applicable law or agreed to in writing, software\r\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
    "# See the License for the specific language governing permissions and\r\n",
    "# limitations under the License.\r\n",
    "\r\n",
    "# Lint as: python3\r\n",
    "\"\"\"Trains TFT based on a defined set of parameters.\r\n",
    "\r\n",
    "Uses default parameters supplied from the configs file to train a TFT model from\r\n",
    "scratch.\r\n",
    "\r\n",
    "Usage:\r\n",
    "python3 script_train_fixed_params {expt_name} {output_folder}\r\n",
    "\r\n",
    "Command line args:\r\n",
    "  expt_name: Name of dataset/experiment to train.\r\n",
    "  output_folder: Root folder in which experiment is saved\r\n",
    "\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "import argparse\r\n",
    "import datetime as dte\r\n",
    "import os\r\n",
    "\r\n",
    "import data_formatters.base\r\n",
    "import expt_settings.configs\r\n",
    "import libs.hyperparam_opt\r\n",
    "import libs.tft_model_2\r\n",
    "import libs.utils as utils\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import data_formatters.daf_extended\r\n",
    "\r\n",
    "ExperimentConfig = expt_settings.configs.ExperimentConfig\r\n",
    "HyperparamOptManager = libs.hyperparam_opt.HyperparamOptManager\r\n",
    "ModelClass = libs.tft_model_2.TemporalFusionTransformer\r\n",
    "DafFormatter = data_formatters.daf_extended.DafExtendedFormatter()\r\n",
    "\r\n",
    "\r\n",
    "def main(expt_name,\r\n",
    "         model_folder,\r\n",
    "         data_csv_path,\r\n",
    "         data_formatter,\r\n",
    "         use_testing_mode=False):\r\n",
    "  \"\"\"Trains tft based on defined model params.\r\n",
    "\r\n",
    "  Args:\r\n",
    "    expt_name: Name of experiment\r\n",
    "    model_folder: Folder path where models are serialized\r\n",
    "    data_csv_path: Path to csv file containing data\r\n",
    "    data_formatter: Dataset-specific data fromatter (see\r\n",
    "      expt_settings.dataformatter.GenericDataFormatter)\r\n",
    "    use_testing_mode: Uses a smaller models and data sizes for testing purposes\r\n",
    "      only -- switch to False to use original default settings\r\n",
    "  \"\"\"\r\n",
    "\r\n",
    "  num_repeats = 1\r\n",
    "\r\n",
    "  if not isinstance(data_formatter, data_formatters.base.GenericDataFormatter):\r\n",
    "    raise ValueError(\r\n",
    "        \"Data formatters should inherit from\" +\r\n",
    "        \"AbstractDataFormatter! Type={}\".format(type(data_formatter)))\r\n",
    "\r\n",
    "  # Tensorflow setup\r\n",
    "  # default_keras_session = tf.keras.backend.get_session()\r\n",
    "\r\n",
    "  # if use_gpu:\r\n",
    "  #   tf_config = utils.get_default_tensorflow_config(tf_device=\"gpu\", gpu_id=0)\r\n",
    "\r\n",
    "  # else:\r\n",
    "  #   tf_config = utils.get_default_tensorflow_config(tf_device=\"cpu\")\r\n",
    "\r\n",
    "  print(\"*** Training from defined parameters for {} ***\".format(expt_name))\r\n",
    "\r\n",
    "  print(\"Loading & splitting data...\")\r\n",
    "  raw_data = pd.read_csv(data_csv_path, index_col=0)\r\n",
    "  train, valid, test = data_formatter.split_data(raw_data)\r\n",
    "  train_samples, valid_samples = data_formatter.get_num_samples_for_calibration(\r\n",
    "  )\r\n",
    "\r\n",
    "  # Sets up default params\r\n",
    "  fixed_params = data_formatter.get_experiment_params()\r\n",
    "  params = data_formatter.get_default_model_params()\r\n",
    "  params[\"model_folder\"] = model_folder\r\n",
    "\r\n",
    "  # Parameter overrides for testing only! Small sizes used to speed up script.\r\n",
    "  if use_testing_mode:\r\n",
    "    fixed_params[\"num_epochs\"] = 1\r\n",
    "    params[\"hidden_layer_size\"] = 5\r\n",
    "    train_samples, valid_samples = 100, 10\r\n",
    "\r\n",
    "  # Sets up hyperparam manager\r\n",
    "  print(\"*** Loading hyperparm manager ***\")\r\n",
    "  opt_manager = HyperparamOptManager({k: [params[k]] for k in params},\r\n",
    "                                     fixed_params, model_folder)\r\n",
    "\r\n",
    "  # Training -- one iteration only\r\n",
    "  print(\"*** Running calibration ***\")\r\n",
    "  print(\"Params Selected:\")\r\n",
    "  for k in params:\r\n",
    "    print(\"{}: {}\".format(k, params[k]))\r\n",
    "\r\n",
    "  best_loss = np.Inf\r\n",
    "  for _ in range(num_repeats):\r\n",
    "\r\n",
    "    # tf.reset_default_graph()\r\n",
    "    # with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\r\n",
    "\r\n",
    "      # tf.keras.backend.set_session(sess)\r\n",
    "\r\n",
    "      params = opt_manager.get_next_parameters()\r\n",
    "      model = ModelClass(params)\r\n",
    "\r\n",
    "      if not model.training_data_cached():\r\n",
    "        model.cache_batched_data(train, \"train\", num_samples=train_samples)\r\n",
    "        model.cache_batched_data(valid, \"valid\", num_samples=valid_samples)\r\n",
    "\r\n",
    "      # sess.run(tf.global_variables_initializer())\r\n",
    "      model.fit()\r\n",
    "\r\n",
    "      val_loss = model.evaluate()\r\n",
    "\r\n",
    "      if val_loss < best_loss:\r\n",
    "        opt_manager.update_score(params, val_loss, model)\r\n",
    "        best_loss = val_loss\r\n",
    "\r\n",
    "      # tf.keras.backend.set_session(default_keras_session)\r\n",
    "\r\n",
    "  print(\"*** Running tests ***\")\r\n",
    "  # tf.reset_default_graph()\r\n",
    "  # with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\r\n",
    "    # tf.keras.backend.set_session(sess)\r\n",
    "  best_params = opt_manager.get_best_params()\r\n",
    "  model = ModelClass(best_params)\r\n",
    "\r\n",
    "  model.load(opt_manager.hyperparam_folder)\r\n",
    "\r\n",
    "  print(\"Computing best validation loss\")\r\n",
    "  val_loss = model.evaluate(valid)\r\n",
    "\r\n",
    "  print(\"Computing test loss\")\r\n",
    "  output_map = model.predict(test, return_targets=True)\r\n",
    "  targets = data_formatter.format_predictions(output_map[\"targets\"])\r\n",
    "  p50_forecast = data_formatter.format_predictions(output_map[\"p50\"])\r\n",
    "  p90_forecast = data_formatter.format_predictions(output_map[\"p90\"])\r\n",
    "\r\n",
    "  def extract_numerical_data(data):\r\n",
    "    \"\"\"Strips out forecast time and identifier columns.\"\"\"\r\n",
    "    return data[[\r\n",
    "        col for col in data.columns\r\n",
    "        if col not in {\"forecast_time\", \"identifier\"}\r\n",
    "    ]]\r\n",
    "\r\n",
    "  p50_loss = utils.numpy_normalised_quantile_loss(\r\n",
    "      extract_numerical_data(targets), extract_numerical_data(p50_forecast),\r\n",
    "      0.5)\r\n",
    "  p90_loss = utils.numpy_normalised_quantile_loss(\r\n",
    "      extract_numerical_data(targets), extract_numerical_data(p90_forecast),\r\n",
    "      0.9)\r\n",
    "\r\n",
    "    # tf.keras.backend.set_session(default_keras_session)\r\n",
    "\r\n",
    "  print(\"Training completed @ {}\".format(dte.datetime.now()))\r\n",
    "  print(\"Best validation loss = {}\".format(val_loss))\r\n",
    "  print(\"Params:\")\r\n",
    "\r\n",
    "  for k in best_params:\r\n",
    "    print(k, \" = \", best_params[k])\r\n",
    "  print()\r\n",
    "  print(\"Normalised Quantile Loss for Test Data: P50={}, P90={}\".format(\r\n",
    "      p50_loss.mean(), p90_loss.mean()))\r\n",
    "\r\n",
    "  return output_map\r\n",
    "\r\n",
    "\r\n",
    "model = main(\r\n",
    "      expt_name='daf',\r\n",
    "      model_folder='tft_outputs/saved_models/daf/fixed',\r\n",
    "      data_csv_path='tft_outputs/data/daf/tf_input.csv',\r\n",
    "      data_formatter=DafFormatter,\r\n",
    "      use_testing_mode=False)  # Change to false to use original default params\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.5.0\n",
      "*** Training from defined parameters for daf ***\n",
      "Loading & splitting data...\n",
      "Processed 0 out of 9461 drivers\n",
      "Processed 1000 out of 9461 drivers\n",
      "Processed 2000 out of 9461 drivers\n",
      "Processed 3000 out of 9461 drivers\n",
      "Processed 4000 out of 9461 drivers\n",
      "Processed 5000 out of 9461 drivers\n",
      "Processed 6000 out of 9461 drivers\n",
      "Processed 7000 out of 9461 drivers\n",
      "Processed 8000 out of 9461 drivers\n",
      "Processed 9000 out of 9461 drivers\n",
      "Setting scalers with training data...\n",
      "*** Loading hyperparm manager ***\n",
      "*** Running calibration ***\n",
      "Params Selected:\n",
      "dropout_rate: 0.2\n",
      "hidden_layer_size: 20\n",
      "learning_rate: 0.001\n",
      "minibatch_size: 64\n",
      "max_gradient_norm: 1.0\n",
      "num_heads: 4\n",
      "stack_size: 1\n",
      "model_folder: tft_outputs/saved_models/daf/fixed\n",
      "Resetting temp folder\n",
      "Cached data \"train\" updated\n",
      "Cached data \"valid\" updated\n",
      "*** Fitting TemporalFusionTransformer ***\n",
      "Getting batched_data\n",
      "Using cached training data\n",
      "Using cached validation data\n",
      "Using keras standard fit\n",
      "Epoch 1/100\n",
      "21418/21418 [==============================] - 2925s 135ms/step - loss: 0.3821 - root_mean_squared_error: 0.7520 - mean_absolute_error: 0.5945 - val_loss: 0.3620 - val_root_mean_squared_error: 0.7150 - val_mean_absolute_error: 0.5633\n",
      "Epoch 2/100\n",
      "21418/21418 [==============================] - 2864s 134ms/step - loss: 0.3695 - root_mean_squared_error: 0.7328 - mean_absolute_error: 0.5755 - val_loss: 0.3587 - val_root_mean_squared_error: 0.7235 - val_mean_absolute_error: 0.5709\n",
      "Epoch 3/100\n",
      "21418/21418 [==============================] - 2891s 135ms/step - loss: 0.3660 - root_mean_squared_error: 0.7276 - mean_absolute_error: 0.5703 - val_loss: 0.3587 - val_root_mean_squared_error: 0.7030 - val_mean_absolute_error: 0.5513\n",
      "Epoch 4/100\n",
      "21418/21418 [==============================] - 2859s 133ms/step - loss: 0.3639 - root_mean_squared_error: 0.7244 - mean_absolute_error: 0.5672 - val_loss: 0.3581 - val_root_mean_squared_error: 0.7071 - val_mean_absolute_error: 0.5538\n",
      "Epoch 5/100\n",
      "21418/21418 [==============================] - 2888s 135ms/step - loss: 0.3623 - root_mean_squared_error: 0.7216 - mean_absolute_error: 0.5647 - val_loss: 0.3575 - val_root_mean_squared_error: 0.7102 - val_mean_absolute_error: 0.5570\n",
      "Epoch 6/100\n",
      "21418/21418 [==============================] - 2941s 137ms/step - loss: 0.3611 - root_mean_squared_error: 0.7196 - mean_absolute_error: 0.5629 - val_loss: 0.3571 - val_root_mean_squared_error: 0.7124 - val_mean_absolute_error: 0.5592\n",
      "Epoch 7/100\n",
      "21418/21418 [==============================] - 30346s 1s/step - loss: 0.3599 - root_mean_squared_error: 0.7177 - mean_absolute_error: 0.5612 - val_loss: 0.3571 - val_root_mean_squared_error: 0.7201 - val_mean_absolute_error: 0.5644\n",
      "Epoch 8/100\n",
      "21418/21418 [==============================] - 3995s 187ms/step - loss: 0.3590 - root_mean_squared_error: 0.7162 - mean_absolute_error: 0.5600 - val_loss: 0.3572 - val_root_mean_squared_error: 0.7155 - val_mean_absolute_error: 0.5605\n",
      "Epoch 9/100\n",
      "21418/21418 [==============================] - 2974s 139ms/step - loss: 0.3583 - root_mean_squared_error: 0.7150 - mean_absolute_error: 0.5589 - val_loss: 0.3586 - val_root_mean_squared_error: 0.7068 - val_mean_absolute_error: 0.5535\n",
      "Epoch 10/100\n",
      "21418/21418 [==============================] - 2948s 138ms/step - loss: 0.3576 - root_mean_squared_error: 0.7135 - mean_absolute_error: 0.5576 - val_loss: 0.3581 - val_root_mean_squared_error: 0.6909 - val_mean_absolute_error: 0.5402\n",
      "Epoch 11/100\n",
      "21418/21418 [==============================] - 2955s 138ms/step - loss: 0.3569 - root_mean_squared_error: 0.7124 - mean_absolute_error: 0.5567 - val_loss: 0.3594 - val_root_mean_squared_error: 0.7158 - val_mean_absolute_error: 0.5602\n",
      "Cannot load from tft_outputs/saved_models/daf/fixed\\tmp, skipping ...\n",
      "Using cached validation data\n",
      "9093/9093 [==============================] - 253s 28ms/step - loss: 0.3594 - root_mean_squared_error: 0.7159 - mean_absolute_error: 0.5602\n",
      "Optimal model found, updating\n",
      "Model saved to: tft_outputs/saved_models/daf/fixed\\TemporalFusionTransformer.ckpt\n",
      "*** Running tests ***\n",
      "Resetting temp folder\n",
      "Loading model from tft_outputs/saved_models/daf/fixed\\TemporalFusionTransformer.ckpt\n",
      "Computing best validation loss\n",
      "9093/9093 [==============================] - 229s 25ms/step - loss: 0.3594 - root_mean_squared_error: 0.7159 - mean_absolute_error: 0.5602\n",
      "Computing test loss\n",
      "Training completed @ 2021-07-26 12:50:02.806234\n",
      "Best validation loss = 0.3593672811985016\n",
      "Params:\n",
      "dropout_rate  =  0.2\n",
      "hidden_layer_size  =  20\n",
      "learning_rate  =  0.001\n",
      "max_gradient_norm  =  1.0\n",
      "minibatch_size  =  64\n",
      "model_folder  =  tft_outputs/saved_models/daf/fixed\n",
      "num_heads  =  4\n",
      "stack_size  =  1\n",
      "total_time_steps  =  15\n",
      "num_encoder_steps  =  14\n",
      "num_epochs  =  100\n",
      "early_stopping_patience  =  5\n",
      "multiprocessing_workers  =  5\n",
      "column_definition  =  [('DRIVERID', <DataTypes.CATEGORICAL: 1>, <InputTypes.ID: 4>), ('END_DATETIME', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>), ('GROSS_WEIGHT', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('TRIP_DISTANCE', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('ALTITUDE_DELTA', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('USED_FUEL', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('FUEL_CONSUMPTION', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('CC_DIST', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('CC_ENABLED', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('BRAKEDURATION', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('DPA_SCORE', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('TIME_STEPS', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('TRUCK_TYPE', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('AXLE_CONF', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('COMMERCIAL_NAME', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('TRUCK_SERIES', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('TRUCK_ENGINE', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('CATEGORICAL_ID', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('DAYOFWEEK', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>), ('HOUROFDAY', <DataTypes.CATEGORICAL: 1>, <InputTypes.OBSERVED_INPUT: 1>)]\n",
      "input_size  =  18\n",
      "output_size  =  1\n",
      "category_counts  =  [2, 5, 12, 3, 3, 9461, 7, 24]\n",
      "input_obs_loc  =  [8]\n",
      "static_input_loc  =  [15]\n",
      "known_regular_inputs  =  [9]\n",
      "known_categorical_inputs  =  [5]\n",
      "\n",
      "Normalised Quantile Loss for Test Data: P50=0.10889389922502175, P90=0.04932496254289274\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "targets = list(model['targets']['t+0'].values)\r\n",
    "p50 = list(model['p50']['t+0'].values)\r\n",
    "p90 = list(model['p90']['t+0'].values)\r\n",
    "p10 = list(model['p10']['t+0'].values)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "target_unscaled = DafFormatter._target_scaler.inverse_transform(targets)\r\n",
    "p50_unscaled = DafFormatter._target_scaler.inverse_transform(p50)\r\n",
    "p90_unscaled = DafFormatter._target_scaler.inverse_transform(p90)\r\n",
    "p10_unscaled = DafFormatter._target_scaler.inverse_transform(p10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n",
    "\r\n",
    "print('RMSE p10: {}'.format(mean_squared_error(targets, p10, squared=False)))\r\n",
    "print('MAE p10: {}'.format(mean_absolute_error(targets, p10)))\r\n",
    "print('RMSE p50: {}'.format(mean_squared_error(targets, p50, squared=False)))\r\n",
    "print('MAE p50: {}'.format(mean_absolute_error(targets, p50)))\r\n",
    "print('RMSE p90: {}'.format(mean_squared_error(targets, p90, squared=False)))\r\n",
    "print('MAE p90: {}'.format(mean_absolute_error(targets, p90)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE p10: 0.8062184305164593\n",
      "MAE p10: 0.657649949352054\n",
      "RMSE p50: 0.49624124053611524\n",
      "MAE p50: 0.37209188725085757\n",
      "RMSE p90: 0.7670339460859574\n",
      "MAE p90: 0.6235325115261202\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n",
    "\r\n",
    "print('RMSE p10 unscaled: {}'.format(mean_squared_error(target_unscaled, p10_unscaled, squared=False)))\r\n",
    "print('MAE p10 unscaled: {}'.format(mean_absolute_error(target_unscaled, p10_unscaled)))\r\n",
    "print('RMSE p50 unscaled: {}'.format(mean_squared_error(target_unscaled, p50_unscaled, squared=False)))\r\n",
    "print('MAE p50 unscaled: {}'.format(mean_absolute_error(target_unscaled, p50_unscaled)))\r\n",
    "print('RMSE p90 unscaled: {}'.format(mean_squared_error(target_unscaled, p90_unscaled, squared=False)))\r\n",
    "print('MAE p90 unscaled: {}'.format(mean_absolute_error(target_unscaled, p90_unscaled)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RMSE p10 unscaled: 14.866357462827747\n",
      "MAE p10 unscaled: 12.126811826560397\n",
      "RMSE p50 unscaled: 9.150497570464788\n",
      "MAE p50 unscaled: 6.861231263014301\n",
      "RMSE p90 unscaled: 14.143811155347052\n",
      "MAE p90 unscaled: 11.49769996280489\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5f23c331dcc0e959a18d97d72b32723b8e8fefc4a029dc171226f15b89161c30"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}